{"name":"Oscar Nights","tagline":"A predictive model for Academy Award performance by Antonio Coppola and Andy Shi, Harvard University.","body":"### Welcome to Oscar Nights\r\nOscar Nights is a project realized for Harvard's Computer Science 109: Data Science. We aim to discover which movie features (budget, director, review statistics, etcâ€¦) are predictive of success on the night of the Academy Awards, and to see how well a classifier can predict Oscar nominations and Oscar winnings. The project comes in the form of an iPython notebook, with associated data files. In order to execute the code, extract the contents of the project archive locally, and then launch iPython.\r\n\r\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube.com/embed/o6YXdTeuDCA\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\r\n\r\n\r\n### Project Specifications\r\nWe build two classifiers: one allows us to predict whether a movie will be *nominated for at least one Oscar*, and the other allows us to predict whether an Oscar nominee *will win at least one Oscar*. Aside from building the predictive model, we are also interested in observing which features happen to be the most important predictors in the model.\r\n\r\nWe train the classifiers on data for over 1000 movies released between 2005 and 2009 from Noah Smith's [Movies Data Corpus](http://www.ark.cs.cmu.edu/movie$-data/). This data includes movie reviews from sources such as the [New York Times](http://www.nytimes.com/pages/movies/index.html) and the [Austin Chronicle](http://www.austinchronicle.com/screens/). In order to quantify the positiveness of the movie reviews (this task is commonly referred to as *sentiment analysis*), we also use [training data](http://www.cs.cornell.edu/people/pabo/movie-review-data/) from Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\r\n\r\n### Results\r\nWe are successfully able to predict Oscar nominations with an accuracy of **95%**, and to predict Oscar winnings with an accuracy of **90%**. Because the proportion of Oscar-winning and Oscar-nominated movies is very low, we also look at the F1 prediction score, a metric bounded between 0 (worst) and 1 (best) that gives an indication of both the true positive rate and the false positive rate. We perform very well still, achieving an F1 score of 0.87 for identifying Oscar nominees, and an F1 score of 0.89 for identifying Oscar winners.\r\n\r\nWe find that the strongest predictors of Academy award performance (both in terms of nominations and award winnings) are the qualities of the movie reviews, box office performance, production budget, running time, number of screens for the movie premiere, and day of the year in which the movie is released. The following graph shows feature importance for nominee prediction:\r\n\r\n![](https://dl.dropboxusercontent.com/u/113867121/features_nominee.png)\r\n\r\nThe following graph shows instead feature importance for winner prediction:\r\n\r\n![](https://dl.dropboxusercontent.com/u/113867121/features_winner.png)\r\n\r\nAs one might expect, we find that Oscar winners and Oscar nominees tend to receive better-than-average reviews.\r\n\r\n![](https://dl.dropboxusercontent.com/u/113867121/reviews.png)\r\n\r\nPerhaps more surprisingly, however, we also find that movies that are released later in the year tend to perform better at the Academy Awards. This effect might be driven by big-budget holiday releases.\r\n\r\n![](https://dl.dropboxusercontent.com/u/113867121/days.png)\r\n\r\n### Contact Us\r\nHave questions about the project? Open an issue on our Github repository and we will be happy to help!\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}